{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Understanding ADMM"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Formulation of the general problem ADMM solves"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to solve an optimization problem, and extended Lasso using ADMM. In order to do that, first we go over the general theory, as introduced in http://www.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "\\text{minimize } f(x) + g(z)\n",
      "\\\\\n",
      "\\text{subject to } Ax + Bz = c\n",
      "\\\\\n",
      "\\text{with variables }x \\in \\mathbb{R}^n\\text{ and }z \\in \\mathbb{R}^m\n",
      "\\\\\n",
      "\\text{where }A \\in \\mathbb{R}^{p \\times n}\\text{, }B \\in \\mathbb{R}^{p \\times m}\\text{ and }c \\in \\mathbb{R}^p\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assuming that f and g are convex.\n",
      "The optimal value of the problem above will be noted by:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$p = \\inf\\{{f (x) + g(z) | Ax + Bz = c}\\}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We form the augmented Lagrangian"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$L_\u03c1 (x, z, y) = f (x) + g(z) + y^T (Ax + Bz \u2212 c) + (\u03c1/2)\\|Ax + Bz \u2212 c\\|_2^2$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ADMM consists of the iterations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "x_{k+1} := arg\\min_x L_\\rho(x, z^k, y^k)\n",
      "\\\\\n",
      "z_{k+1} := arg\\min_z L_\\rho(x^{k+1}, z, y^k)\n",
      "\\\\\n",
      "y_{k+1} := y_k + \\rho(Ax^{k+1} + Bz^{k+1} - c)\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Formulation of a generic lasso using ADMM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below we find a general problem in computer vision and machine learning. It is commongly referred to as LASSO (least absolute shrinkage and selection operator) regression, which uses the constraint that the L1 of the parameter vector, is no greater than a given value. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\\begin{array}{c}\n",
      "arg \\min_\\Delta{0.5\\|\\Psi \u2212 \\Xi \\Delta\\|_2^2+\u03b3\\|\\Delta\\|1}\n",
      "\\\\\n",
      "\\text{where }\n",
      "\\\\\n",
      "\\Psi =\\text{ Input data,}\n",
      "\\\\\n",
      "\\Xi = \\text{Dictionary,}\n",
      "\\\\\n",
      "\\Delta =\\text{ Code}\n",
      "\\\\\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This contains only one variable and at first glance it does not fit the general form described for ADMM. However, if we introduce a dummy variable we can express it as follows"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "arg \\min_\\Delta{0.5\\|\\Psi \u2212 \\Xi \\Delta\\|_2^2+\\gamma\\|\\Omega\\|_1}\n",
      "\\\\\n",
      "\\text{subject to }\\Delta = \\Omega\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Why is this helpful? Because the augmented lagrangian becomes now:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$L(\\Delta,\\Omega,\\Lambda)=0.5\\|\\Psi\u2212\\Xi \\Delta\\|_2^2+\u03b3\\|\\Omega\\|_1+(\\rho/2)\\|\\Delta\u2212\\Omega\\|_2^2+vec(\\Lambda)^Tvec(\\Delta\u2212\\Omega)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The ADMM algorithm consits of the iterations:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\\begin{array}{c}\n",
      "\\Delta^{k+1}= arg \\min_\\Delta{L(\\Delta,\\Omega^k,\\Lambda^k)}\n",
      "\\\\\n",
      "\\Omega^{k+1}= arg \\min_\\Omega{L(\\Delta^{k+1},\\Omega,\\Lambda^k)}\n",
      "\\\\\n",
      "\\Lambda^{k+1}=\\Lambda^k+\\rho(\\Delta\u2212\\Omega)\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The important bit now is to do the comparison between this form and the general ADMM problem. Let the variable equivalence extravaganza begin:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "A = 1\n",
      "\\\\\n",
      "B = -1\n",
      "\\\\\n",
      "c = 0\n",
      "\\\\\n",
      "x = \\Delta\n",
      "\\\\\n",
      "z = \\Omega\n",
      "\\\\\n",
      "y = \\Lambda\n",
      "\\\\\n",
      "f(x) + g(z) = 0.5\\|\\Psi \u2212 \\Xi \\Delta\\|_2^2+\\gamma\\|\\Omega\\|_1\n",
      "\\\\\n",
      "f(x) = 0.5\\|\\Psi \u2212 \\Xi \\Delta\\|_2^2\n",
      "\\\\\n",
      "g(z) = \\gamma\\|\\Omega\\|_1\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's write the first iteration of the solution in terms of the newly defined variables:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$L_\u03c1 (x, z, y) = f (x) + g(z) + y^T (Ax + Bz \u2212 c) + (\u03c1/2)\\|Ax + Bz \u2212 c\\|_2^2$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This becomes:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$L_\u03c1 (x, z, y) = f (x) + g(z) + y^T (x - z) + (\u03c1/2)\\|x - z\\|_2^2$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which is in turn:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$L_\u03c1 (x, z, y) = 0.5\\|\\Psi \u2212 \\Xi X\\|_2^2 + \\gamma\\|Z|_1 + Y^T (X - Z) + (\u03c1/2)\\|X - Z\\|_2^2$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The only variables here are X, Y and Z. Let's figure out what each step looks like:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "x_{k+1} := arg\\min_x L_\\rho(x, z^k, y^k) = arg\\min_x{ 0.5\\|\\Psi \u2212 \\Xi X\\|_2^2 + Y^T (X - Z) + (\u03c1/2)\\|X - Z\\|_2^2}\n",
      "\\\\\n",
      "z_{k+1} := arg\\min_z L_\\rho(x^{k+1}, z, y^k) = arg\\min_z{\\gamma\\|Z|_1 + Y^T (X - Z) + (\u03c1/2)\\|X - Z\\|_2^2}\n",
      "\\\\\n",
      "y_{k+1} := y_k + \\rho(Ax^{k+1} + Bz^{k+1} - c) = y_k + \\rho(x^{k+1} - Bz^{k+1})\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sub-problem 1: Solving for X"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since we do not have a term on L1 anymore, according to the literature, we find that the best way to solve the subproblem for X is to use least squares, the following way:\n",
      "http://www.simonlucey.com/lasso-using-admm/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "x_{k+1} := arg\\min_x{ 0.5\\|\\Psi \u2212 \\Xi X\\|_2^2 + Y^T (X - Z) + (\u03c1/2)\\|X - Z\\|_2^2}\n",
      "\\\\\n",
      "X = lstsq( \\Xi^T*\\Xi+\\rho*I , \\Xi^T*\\Psi + \\rho*Z - Y )$$"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Note: Ask Simon Lucey how he goes from the first line to the second."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Subproblem 2: Solving for Z"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, the equivalent problem for Z does not have a matrix multiplying the main variable, and can be efficiently solved using only soft threshold:\n",
      "http://www.simonlucey.com/soft-thresholding/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "z_{k+1} := arg\\min_z{\\gamma\\|Z|_1 + Y^T (X - Z) + (\u03c1/2)\\|X - Z\\|_2^2}\n",
      "\\\\\n",
      "Z = softthreshold(\\gamma\\|Z|_1 + Y^T (X - Z) + (\u03c1/2)\\|X - Z\\|_2^2)\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sub-problem 3: Solving for Y"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a simple updating step."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "y_{k+1} := y_k + \\rho(x^{k+1} - Bz^{k+1})\n",
      "\\\\\n",
      "Y = Y_{old} + \\rho(X - Z)\n",
      "$$"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}