{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the paper titled Online Detection of Unusual Events in Videos via Dynamic Sparse Coding (CVPR2011) by Bin Zhao and Li Fei Fei we find the following Lasso problem as equation (4). Let's find a solution for it using ADMM.\n",
      "\n",
      "CVPR2011 paper: http://vision.stanford.edu/pdf/ZhaoFeiFeiXing_CVPR2011.pdf\u200e\n",
      "\n",
      "ADMM  paper: http://www.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Problem:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "J(X_i, \\alpha_i, D) = \\min_{\\alpha_{i}^{1},...,\\alpha_{i}^{n_i}}{\\frac{1}{2}\\sum_{j}\\left \\| x_i^j - D\\alpha_i^j \\right \\|_2^2 + \\lambda_1 \\sum_j{\\left \\| \\alpha_i^j \\right \\|_1} + \\lambda_2 \\sum_{j,k}{W_{j,k}\\left \\| \\alpha_i^j - \\alpha_i^k \\right \\|_2^2}}\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Solution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Xi are input data, it is constant so we can take it out of the optimization function:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "J(\\alpha_i, D) = \\min_{\\alpha_{i}^{1},...,\\alpha_{i}^{n_i}}{\\frac{1}{2}\\sum_{j}\\left \\| x_i^j - D\\alpha_i^j \\right \\|_2^2 + \\lambda_1 \\sum_j{\\left \\| \\alpha_i^j \\right \\|_1} + \\lambda_2 \\sum_{j,k}{W_{j,k}\\left \\| \\alpha_i^j - \\alpha_i^k \\right \\|_2^2}}\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, let's flatten and vectorize the problem. For this we will stack all the alphas, and find a suitable variable substitution that is also a vector instead of a matrix (diagonal matrices and a Kroenecker will be the tricks used):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "S_{jj} =  \\sum_k{W_{j,k}}\\text{ , for }J \\not= k\\text{ , }S_{j,k} = 0\n",
      "\\\\\n",
      "H_{j,k} = S_{j,k} - W_{j,k}\n",
      "\\\\\n",
      "H_{expand} = H \\otimes eye(dim(D)) \n",
      "\\\\\n",
      "\\lambda_2 \\sum_{j,k}{W_{j,k}\\left \\| \\alpha_i^j - \\alpha_i^k \\right \\|_2^2} = \\lambda_2 A' *H_{expand} * A\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Similarly, we can express the l2 and l1 norms in the following way:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\frac{1}{2}\\sum_{j}\\left \\| x_i^j - D\\alpha_i^j \\right \\|_2^2 = \\frac{1}{2} \\left \\| X - DA \\right \\|_2^ 2\n",
      "\\\\\n",
      "\\lambda_1 \\sum_j{\\left \\| \\alpha^j \\right \\|_1} = \\lambda_1 \\left \\| A \\right \\|_1 \n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With all these replacements, our optimization function will end up looking like:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "J(A, D) = \\min_{A}{\\frac{1}{2}\\left \\| X - DA \\right \\|_2^2 + \\lambda_1 \\left \\| A \\right \\|_1 + \\lambda_2  A^THA  }\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That is our sparse coding function and we can solve it using an alternation between solving for the dictionary (D) and solving for the code (A)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Solving for A:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "J(A) = \\min_{A}{\\frac{1}{2}\\left \\| X - DA \\right \\|_2^2 + \\lambda_1 \\left \\| A \\right \\|_1 + \\lambda_2  A^THA }\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since the l1 norm (second term) is not derivable, we need to introduce a dual variable (Z) to separate the problem:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$A = Z$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then the problem becomes:\n",
      "$$\n",
      "J(A, Z) = \\min_{A}{\\frac{1}{2}\\left \\| X - DA \\right \\|_2^2 + \\lambda_1 \\left \\| Z \\right \\|_1 + \\lambda_2  A^THA }\n",
      "\\\\\n",
      "\\text{subject to}\n",
      "\\\\\n",
      "A = Z\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to add the restriction to the objective function, let's add a third variable, Y (a langrange multiplier)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}