{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Adapting Zhao's equation for ADMM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In  this exercise we will find a suitable representation for the following problem in order to solve it with ADMM."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Zhao's email"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Hi, Juan Carlos\n",
      "\n",
      "Yes, I modified the feature sign a little bit to fit the problem in (4). "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "J(X_i, \\alpha_i, D) = \\min_{\\alpha_{i}^{1},...,\\alpha_{i}^{n_i}}{\\frac{1}{2}\\sum_{j}\\left \\| x_i^j - D\\alpha_i^j \\right \\|_2^2 + \\lambda_1 \\sum_j{\\left \\| \\alpha^j \\right \\|_1} + \\lambda2 \\sum{W_{j,k}\\left \\| \\alpha_i^j - \\alpha_i^k \\right \\|_2^2}}\n",
      "\\end{array}\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "(Ariel) I think there is a missing i subindex, since the Jacobian is a function of \\alpha_i, let's fix the term with \\lambda1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "J(X_i, \\alpha_i, D) = \\min_{\\alpha_{i}^{1},...,\\alpha_{i}^{n_i}}{\\frac{1}{2}\\sum_{j}\\left \\| x_i^j - D\\alpha_i^j \\right \\|_2^2 + \\lambda_1 \\sum_j{\\left \\| \\alpha_i^j \\right \\|_1} + \\lambda2 \\sum{W_{j,k}\\left \\| \\alpha_i^j - \\alpha_i^k \\right \\|_2^2}}\n",
      "\\end{array}\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "(Ariel) Let's remove the i subindex from the Jacobian, it is already known that this calculation is done per cuboid."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\\begin{array}{c}\n",
      "J(X, \\alpha, D) = \\min_{\\alpha^{1},...,\\alpha^{n}}{\\frac{1}{2}\\sum_{j}\\left \\| x^j - D\\alpha^j \\right \\|_2^2 + \\lambda_1 \\sum_j{\\left \\| \\alpha^j \\right \\|_1} + \\lambda2 \\sum{W_{j,k}\\left \\| \\alpha^j - \\alpha^k \\right \\|_2^2}}\n",
      "\\end{array}\n",
      "\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "First, I define graph laplacian as L=S-W, where S is the diagonal matrix with S_jj = sum_k W_{jk}."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "L = S - W\n",
      "\\\\\n",
      "S_{jj} = \\sum_k{W_{j,k}}\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Then I define an augmented version of \\alpha, by stacking all \\alpha vectors together to get a much higher dimensional vector A. For example, if each \\alpha is d dimensional, and there are n \\alpha in total, this new higher-dimensional vector will be nd \\times 1."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "A = reshape(\\alpha)\n",
      "$$"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Then we augment the graph laplacian, by using \n",
      "L_expand = sparse(kron(L,eye(size(D,2))));"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "L_{expand} = L \\otimes eye(dim(D)) \n",
      "$$"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "After such augment, the last term in (4) will be much simpler, i.e., A' * L_expand * A. You can also reform the first term using the newly defined A vector. Then you could compute gradient of the smooth part in (4) w.r.t. A easily, and use this in feature sign algorithm. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\sum{W_{j,k}\\left \\| \\alpha^j - \\alpha^k \\right \\|_2^2} = A' *L_{expand} * A\n",
      "$$"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Then the first argument would be equivalent to:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "X := x_i \\text{ all stacked to create a single dimensional vector}\n",
      "\\\\\n",
      "C = X - DA\n",
      "\\\\\n",
      "\\frac{1}{2}\\sum_{j}\\left \\| x^j - D\\alpha^j \\right \\|_2^2 = C'C \n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "On a side note, at the time I was working on this paper, feature sign search is one of the fastest algorithm for solving L1 problem. But now, ADMM is much better and easier to formulate. \n",
      "\n",
      "Thanks\n",
      "Bin"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Formulation of the general problem ADMM solves"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "\\text{minimize } f(x) + g(z)\n",
      "\\\\\n",
      "\\text{subject to } Ax + Bz = c\n",
      "\\\\\n",
      "\\text{with variables }x \\in \\mathbb{R}^n\\text{ and }z \\in \\mathbb{R}^m\n",
      "\\\\\n",
      "\\text{where }A \\in \\mathbb{R}^{p \\times n}\\text{, }B \\in \\mathbb{R}^{p \\times m}\\text{ and }c \\in \\mathbb{R}^p\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assuming that f and g are convex.\n",
      "The optimal value of the problem above will be noted by:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$p = \\inf\\{{f (x) + g(z) | Ax + Bz = c}\\}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We form the augmented Lagrangian"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$L_\u03c1 (x, z, y) = f (x) + g(z) + y^T (Ax + Bz \u2212 c) + (\u03c1/2)\\|Ax + Bz \u2212 c\\|_2^2$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ADMM consists of the iterations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "x_{k+1} := arg\\min_x L_\\rho(x, z^k, y^k)\n",
      "\\\\\n",
      "z_{k+1} := arg\\min_z L_\\rho(x^{k+1}, z, y^k)\n",
      "\\\\\n",
      "y_{k+1} := y_k + \\rho(Ax^{k+1} + Bz^{k+1} - c)\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Formulation of a generic lasso using ADMM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below we find a general problem in computer vision and machine learning. It is commongly referred to as LASSO (least absolute shrinkage and selection operator) regression, which uses the constraint that the L1 of the parameter vector, is no greater than a given value. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\\begin{array}{c}\n",
      "arg \\min_\\Delta{0.5\\|\\Psi \u2212 \\Xi \\Delta\\|_2^2+\u03b3\\|\\Delta\\|1}\n",
      "\\\\\n",
      "\\text{where }\n",
      "\\\\\n",
      "\\Psi =\\text{ Input data,}\n",
      "\\\\\n",
      "\\Xi = \\text{Dictionary,}\n",
      "\\\\\n",
      "\\Delta =\\text{ Code}\n",
      "\\\\\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This contains only one variable and at first glance it does not fit the general form described for ADMM. However, if we introduce a dummy variable we can express it as follows"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "arg \\min_\\Delta{0.5\\|\\Psi \u2212 \\Xi \\Delta\\|_2^2+\\gamma\\|\\Omega\\|_1}\n",
      "\\\\\n",
      "\\text{subject to }\\Delta = \\Omega\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Why is this helpful? Because the augmented lagrangian becomes now:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$L(\\Delta,\\Omega,\\Lambda)=0.5\\|\\Psi\u2212\\Xi \\Delta\\|_2^2+\u03b3\\|\\Omega\\|_1+(\\rho/2)\\|\\Delta\u2212\\Omega\\|_2^2+vec(\\Lambda)^Tvec(\\Delta\u2212\\Omega)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The ADMM algorithm consits of the iterations:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\\begin{array}{c}\n",
      "\\Delta^{k+1}= arg \\min_\\Delta{L(\\Delta,\\Omega^k,\\Lambda^k)}\n",
      "\\\\\n",
      "\\Omega^{k+1}= arg \\min_\\Omega{L(\\Delta^{k+1},\\Omega,\\Lambda^k)}\n",
      "\\\\\n",
      "\\Lambda^{k+1}=\\Lambda^k+\\rho(\\Delta\u2212\\Omega)\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The important bit now is to do the comparison between this form and the general ADMM problem. Let the variable equivalence extravaganza begin:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{array}{c}\n",
      "A = 1\n",
      "\\\\\n",
      "B = -1\n",
      "\\\\\n",
      "c = 0\n",
      "\\\\\n",
      "x = \\Delta\n",
      "\\\\\n",
      "z = \\Omega\n",
      "\\\\\n",
      "y = \\Lambda\n",
      "\\\\\n",
      "f(x) + g(z) = 0.5\\|\\Psi \u2212 \\Xi \\Delta\\|_2^2+\\gamma\\|\\Omega\\|_1\n",
      "\\\\\n",
      "f(x) = 0.5\\|\\Psi \u2212 \\Xi \\Delta\\|_2^2\n",
      "\\\\\n",
      "g(z) = \\gamma\\|\\Omega\\|_1\n",
      "\\end{array}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "La siguiente es la solucion\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "$$\\Delta = (\\Xi^T*\\Xi+\\rho*I)  (\\Xi^T*\\Psi + \\rho*\\Omega - \\Lambda)$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}